# status = error
# name = CustomSparkLogConfig

# # Un seul appender console
# appender.console.type = Console
# appender.console.name = STDOUT
# appender.console.target = SYSTEM_OUT
# appender.console.layout.type = PatternLayout
# appender.console.layout.pattern = %m%n

# # Logger racine : seulement ERROR (pas de INFO / DEBUG / WARN)
# rootLogger.level = error
# rootLogger.appenderRefs = stdout
# rootLogger.appenderRef.stdout.ref = STDOUT

# # Réduire encore certains loggers Spark très bavards (optionnel)
# logger.spark.name = org.apache.spark
# logger.spark.level = error
# logger.spark.additivity = false
# logger.spark.appenderRefs = stdout
# logger.spark.appenderRef.stdout.ref = STDOUT

# logger.hadoop.name = org.apache.hadoop
# logger.hadoop.level = error
# logger.hadoop.additivity = false
# logger.hadoop.appenderRefs = stdout
# logger.hadoop.appenderRef.stdout.ref = STDOUT

# # Cacher spécifiquement les erreurs de ShutdownHookManager (le gros stacktrace de fin)
# logger.shutdownhook.name = org.apache.spark.util.ShutdownHookManager
# logger.shutdownhook.level = fatal
# logger.shutdownhook.additivity = false
# logger.shutdownhook.appenderRefs = stdout
# logger.shutdownhook.appenderRef.stdout.ref = STDOUT
